# moves old data set to appropriate pre files

#!/bin/bash

#PBS -l nodes=1:ppn=1,walltime=100:00:00,mem=25gb
#PBS -N move_dumped_dataset
#PBS -V
#PBS -S /bin/bash
#PBS -M ${LOGNAME}@nyu.edu
#PBS -m bae
#PBS -j oe
#PBS -o localhost:${HOME}/jobs/${PBS_JOBNAME}.${PBS_JOBID}.oe

module load mongodb/3.2.8 

echo "Running script..."
# make a copy of the dump
cp -r "$1" "$(dirname "$1")/$(basename "$1")_copy"

#set our first variable to that input
folderpath="$(dirname "$1")/$(basename "$1")_copy/"

# remove indexes
rm system.indexes.json

# rename files
mv "$folderpath"smapp_metadata.json "$folderpath"metadata_pre.json
mv "$folderpath"tweets_limits.json "$folderpath"metadata_limits_pre.json
mv "$folderpath"tweets_filter_criteria.json "$folderpath"filters_pre.json
mv "$folderpath"check_dump_integrity.log "$folderpath"check_dump_integrity_pre.log
rename tweets $2_pre "$folderpath"tweets_*.json "$folderpath"tweets.json "$folderpath"data.json

# compress
bzip2 "$folderpath"*.json

# copy files to scratch
cp *.bz2 /scratch/olympus/$2/data/
cp "$folderpath"check_dump_integrity_pre.log /scratch/olympus/$2/metadata/
cp "$folderpath"metadata_limits_pre.json /scratch/olympus/$2/metadata/
cp "$folderpath"metadata_pre.json /scratch/olympus/$2/metadata/
cp "$folderpath"filters_pre.json /scratch/olympus/$2/filters/
echo "Done"

# https://wikis.nyu.edu/display/NYUHPC/Writing+and+submitting+a+job

